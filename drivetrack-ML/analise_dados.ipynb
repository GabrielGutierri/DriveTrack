{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analise de dados - Agressividade condutores de veiculos\n",
    "\n",
    "\n",
    "## Coleta de Dados\n",
    "- Com os dados salvos dentro do Fiware, é preciso ler cada atributo para cada entidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lê todos os dados que existem para uma entidade, e junta todos os atributos em um dataframe. Preciso modificar a entidade na mão\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "\n",
    "    \n",
    "def fetch_paginated_data(url_base, attribute_name, h_limit=100, is_location=False):\n",
    "    \"\"\"\n",
    "    Busca dados usando paginação com hLimit e hOffset.\n",
    "    \"\"\"\n",
    "    dict_data = {attribute_name: [], 'recvTime_' + attribute_name: []}\n",
    "    h_offset = 0\n",
    "\n",
    "    while True:\n",
    "        # Construir URL com hLimit e hOffset\n",
    "        url = f\"{url_base}?hLimit={h_limit}&hOffset={h_offset}\"\n",
    "        headers = {\n",
    "            'fiware-service': 'smart',\n",
    "            'fiware-servicepath': '/'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            values = data['contextResponses'][0]['contextElement']['attributes'][0]['values']\n",
    "            if not values:\n",
    "                break  # Interrompe se não houver mais valores\n",
    "\n",
    "            for value in values:\n",
    "                dict_data[attribute_name].append(\n",
    "                    value['attrValue']['coordinates'] if is_location else value['attrValue']\n",
    "                )\n",
    "                dict_data['recvTime_' + attribute_name].append(value['recvTime'])\n",
    "\n",
    "            # Incrementa o offset para a próxima página\n",
    "            h_offset += h_limit\n",
    "        else:\n",
    "            print(f\"Erro na requisição: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(dict_data)\n",
    "\n",
    "\n",
    "# Definir atributos e URLs\n",
    "attributes = [\n",
    "    (\"velocidade\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/velocidade\"),\n",
    "    (\"rpm\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/rpm\"),\n",
    "    (\"temperatura\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/temperature\"),\n",
    "    (\"pressao\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/pressure\"),\n",
    "    (\"dataColetaDados\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/dataColetaDados\"),\n",
    "    (\"acelerometroEixoX\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/acelerometroEixoX\"),\n",
    "    (\"acelerometroEixoY\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/acelerometroEixoY\"),\n",
    "    (\"acelerometroEixoZ\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/acelerometroEixoZ\"),\n",
    "    (\"giroscopioPitch\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/giroscopioPitch\"),\n",
    "    (\"giroscopioYaw\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/giroscopioYaw\"),\n",
    "    (\"giroscopioRow\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/giroscopioRow\"),\n",
    "    (\"location\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/location\"),\n",
    "    (\"engineload\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/engineload\"),\n",
    "    (\"idCorrida\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/idCorrida\"),\n",
    "    (\"throttlePosition\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:eduardo:001/attributes/throttlePosition\"),\n",
    "]\n",
    "\n",
    "# DataFrames para armazenar os resultados\n",
    "dfs = {attribute: pd.DataFrame() for attribute, _ in attributes}\n",
    "\n",
    "# Requisições paralelas para todos os atributos\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future_to_attribute = {\n",
    "        executor.submit(fetch_paginated_data, url, attribute, 100, attribute == \"location\"): attribute\n",
    "        for attribute, url in attributes\n",
    "    }\n",
    "    for future in future_to_attribute:\n",
    "        attribute = future_to_attribute[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            dfs[attribute] = result\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao buscar {attribute}: {e}\")\n",
    "\n",
    "# Combinar os DataFrames baseando-se nos timestamps\n",
    "df = dfs[\"velocidade\"]\n",
    "for attribute in [attr for attr, _ in attributes if attr != \"velocidade\"]:\n",
    "    df = df.merge(\n",
    "        dfs[attribute],\n",
    "        how=\"outer\",\n",
    "        left_on=\"recvTime_velocidade\",\n",
    "        right_on=f\"recvTime_{attribute}\"\n",
    "    )\n",
    "\n",
    "# Limpar o DataFrame final\n",
    "\n",
    "df = df.drop(columns=['recvTime_velocidade', 'recvTime_rpm', 'recvTime_temperatura', 'recvTime_pressao', 'recvTime_dataColetaDados', 'recvTime_acelerometroEixoX', 'recvTime_acelerometroEixoY', 'recvTime_acelerometroEixoZ','recvTime_giroscopioPitch', 'recvTime_giroscopioYaw', 'recvTime_giroscopioRow', 'recvTime_location', 'recvTime_engineload', 'recvTime_idCorrida', 'recvTime_throttlePosition'])\n",
    "df = df.dropna()\n",
    "\n",
    "# Exibir o resultado\n",
    "df_raw = df.copy()\n",
    "\n",
    "df_raw\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opcional - Forma de coleta a partir de arquivos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# URL do arquivo JSON\n",
    "file_path = r\"C:\\Users\\vidal\\Downloads\\TCC\\ClassificacaoCondutores\\DadosConsiderados\\EricNormal.txt\"\n",
    "\n",
    "# Lista para armazenar os dados\n",
    "data_list = []\n",
    "\n",
    "# Ler o conteúdo do arquivo JSON linha por linha\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:  # Verifica se a linha não está vazia\n",
    "            try:\n",
    "                data_dict = json.loads(line)\n",
    "                data_list.append({\n",
    "                    'velocidade': float(data_dict['velocidade']['value']),\n",
    "                    'rpm': float(data_dict['rpm']['value']),\n",
    "                    'pressao': float(data_dict['pressure']['value']),\n",
    "                    'temperatura': float(data_dict['temperature']['value']),\n",
    "                    'engineload': float(data_dict['engineload']['value']),\n",
    "                    'throttlePosition': float(data_dict['throttlePosition']['value']),\n",
    "                    'location': data_dict['location']['value']['coordinates'],\n",
    "                    'acelerometroEixoX': float(data_dict['acelerometroEixoX']['value']),\n",
    "                    'acelerometroEixoY': float(data_dict['acelerometroEixoY']['value']),\n",
    "                    'acelerometroEixoZ': float(data_dict['acelerometroEixoZ']['value']),\n",
    "                    'giroscopioRow': float(data_dict['giroscopioRow']['value']),\n",
    "                    'giroscopioPitch': float(data_dict['giroscopioPitch']['value']),\n",
    "                    'giroscopioYaw': float(data_dict['giroscopioYaw']['value']),\n",
    "                    'dataColetaDados': data_dict['dataColetaDados']['value']\n",
    "                })\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Erro ao decodificar JSON na linha: {line}\")\n",
    "\n",
    "# Criar um DataFrame a partir da lista de dados\n",
    "df_raw = pd.DataFrame(data_list)\n",
    "\n",
    "# Exibir o DataFrame\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento de dados \n",
    "\n",
    "A partir do dataset coletado, é preciso fazer algumas correções. \n",
    "\n",
    "- Converter as colunas para cada tipo\n",
    "- Retirar casos inconsistentes. Exemplo: data = 0, location[0,0], dados com -999\n",
    "- Encontrar as corridas que existem nas entidades\n",
    "- Separar o location em lat e log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "df = df_raw\n",
    "\n",
    "#Conversão do tipo de dado.\n",
    "columns = {\n",
    "    'idCorrida': int,\n",
    "    'velocidade': int,\n",
    "    'rpm': float,\n",
    "    'temperatura': int,\n",
    "    'pressao': int,\n",
    "    'engineload': float,\n",
    "    'throttlePosition': float,\n",
    "    'giroscopioYaw': float,\n",
    "    'giroscopioPitch': float,\n",
    "    'giroscopioRow': float,\n",
    "    'acelerometroEixoX': float,\n",
    "    'acelerometroEixoY': float,\n",
    "    'acelerometroEixoZ': float,\n",
    "    'dataColetaDados': datetime,\n",
    "    'location': str,\n",
    "}\n",
    "\n",
    "#Retirar casos em que o dia veio como 0. É uma inconsistencia geral\n",
    "df = df[df['dataColetaDados'] != '0']\n",
    "\n",
    "\n",
    "for column, dtype in columns.items():\n",
    "    if dtype == datetime:\n",
    "        df[column] = pd.to_datetime(df[column])\n",
    "    else:\n",
    "        df[column] = df[column].astype(dtype)\n",
    "\n",
    "df = df.set_index('dataColetaDados')\n",
    "\n",
    "# Remover valores nulos ou inconsistentes em 'location'\n",
    "df = df[df['location'].notna() & df['location'].str.startswith('[')]\n",
    "\n",
    "# Separar latitude e longitude usando split\n",
    "df['latitude'] = df['location'].apply(lambda x: x.strip('[]').split(', ')[0]).astype(float)\n",
    "df['longitude'] = df['location'].apply(lambda x: x.strip('[]').split(', ')[1]).astype(float)\n",
    "\n",
    "#Inconsistência de longitude e latitude serem 0\n",
    "df = df[(df['latitude'] != 0) & (df['longitude'] != 0)]\n",
    "\n",
    "#Removendo valores extremos. Normalmente eles valem -999\n",
    "df = df[(df['velocidade'] != -999) & (df['rpm'] != -999) & (df['pressao'] != -999) & (df['temperatura'] != -999) & (df['engineload'] != -999) & (df['throttlePosition'] != -999)]\n",
    "\n",
    "#Considero um unico dataframe, pois o processo é mais linear\n",
    "df_processado = df[['velocidade', 'rpm', 'temperatura', 'pressao', 'engineload', 'throttlePosition', 'location', 'latitude', 'longitude', 'idCorrida', 'giroscopioYaw', 'giroscopioPitch', \\\n",
    "                    'giroscopioRow', 'acelerometroEixoX', 'acelerometroEixoY', 'acelerometroEixoZ']].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento de Dados - Correção de coordenada\n",
    "- Correção de latitude e longitude usando API google Snap to Roads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar o índice em uma coluna e resetar o índice\n",
    "df_processado = df_processado.reset_index()\n",
    "\n",
    "# Renomear a coluna do índice, se necessário\n",
    "df_processado = df_processado.rename(columns={'index': 'novo_indice'})\n",
    "\n",
    "# Chave para acesso a API do Google Maps. Substitua pela sua chave\n",
    "api_key = \n",
    "\n",
    "# Número máximo de coordenadas por batch\n",
    "batch_size = 100\n",
    "new_coords = {}\n",
    "new_place_ids = {}\n",
    "\n",
    "# Iterar sobre o DataFrame em batches\n",
    "for start in range(0, len(df_processado), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch = df_processado.iloc[start:end]\n",
    "\n",
    "    # Criar o path para a requisição\n",
    "    path = '|'.join([f\"{lat},{lon}\" for lat, lon in zip(batch['latitude'], batch['longitude'])])\n",
    "\n",
    "    # Fazer a requisição para o Google Maps API\n",
    "    url = f\"https://roads.googleapis.com/v1/snapToRoads?path={path}&key={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    snapped_points = response.json()\n",
    "\n",
    "    # Extrair as novas coordenadas e placeIds, ajustando o índice para o DataFrame original\n",
    "    for point in snapped_points.get('snappedPoints', []):\n",
    "        relative_index = point['originalIndex']  # Índice dentro do batch (0-99)\n",
    "        original_index = relative_index + start  # Índice absoluto no DataFrame original\n",
    "        # Adicionar nova coordenada\n",
    "        new_coords[original_index] = (\n",
    "            point['location']['latitude'], \n",
    "            point['location']['longitude']\n",
    "        )\n",
    "        # Adicionar placeId, se disponível\n",
    "        if 'placeId' in point:\n",
    "            new_place_ids[original_index] = point['placeId']\n",
    "\n",
    "# Adicionar as novas coordenadas ao DataFrame original\n",
    "df_processado['new_latitude'] = df_processado.index.map(lambda idx: new_coords[idx][0] if idx in new_coords else None)\n",
    "df_processado['new_longitude'] = df_processado.index.map(lambda idx: new_coords[idx][1] if idx in new_coords else None)\n",
    "\n",
    "# Adicionar a coluna de placeId ao DataFrame original\n",
    "df_processado['placeId'] = df_processado.index.map(lambda idx: new_place_ids[idx] if idx in new_place_ids else None)\n",
    "\n",
    "# Exibir o DataFrame atualizado\n",
    "df_processado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critérios - Limite de Velocidade\n",
    "\n",
    "- Por conta do baixo limite de requisições gratuitas da API da Azure, é preciso reduzir as requisições. Os 3 blocos abaixo avaliam limites de velocidade já coletados, e só requisitão os novos casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linhas que não receberam novas coordenadas são removidas\n",
    "df_processado = df_processado.dropna(subset=['new_latitude', 'new_longitude'])\n",
    "\n",
    "#Parquet que armazena placeId e o limite de velocidade\n",
    "df_limites_cache = pd.read_parquet(\"limites_velocidade.parquet\")\n",
    "\n",
    "#Verificando se existem novos limites de velocidade a ser capturado\n",
    "df_limites_faltantes = df_processado.merge(df_limites_cache, on='placeId', how='left')\n",
    "df_limites_faltantes = df_limites_faltantes[df_limites_faltantes['limite_velocidade'].isna()]\n",
    "\n",
    "if(len(df_limites_faltantes) > 0):\n",
    "    \n",
    "    #Criar um novo parquet com os novos limites\n",
    "    df_limites_faltantes = df_limites_faltantes.drop_duplicates(subset=['placeId'], ignore_index=True)\n",
    "    print(len(df_limites_faltantes))\n",
    "    df_limites_faltantes = df_limites_faltantes.drop(columns=['limite_velocidade', 'Street'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilizando API Azure para encontrar limite da via ( Somente os casos que ainda não tem limite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(df_limites_faltantes) > 0):\n",
    "\n",
    "    import json\n",
    "\n",
    "    # Chave para acesso a API do Azure. Substitua pela sua chave\n",
    "    key =\n",
    "    \n",
    "    # Definindo o URL do endpoint batch\n",
    "    url = f\"https://atlas.microsoft.com/search/address/reverse/batch/json?api-version=1.0&subscription-key={key}\"\n",
    "\n",
    "    # Criando o payload dinamicamente a partir do DataFrame. Não precisa ter limite de 100 coordenadas para a API da Azure\n",
    "    batch_items = []\n",
    "\n",
    "    #Precisa resetar o index mais uma vez\n",
    "    # Transformar o índice em uma coluna e resetar o índice\n",
    "    #df_limites_faltantes = df_limites_faltantes.reset_index()\n",
    "\n",
    "    # Renomear a coluna do índice, se necessário\n",
    "    #df_limites_faltantes = df_limites_faltantes.rename(columns={'index': 'novo_indice'})\n",
    "\n",
    "    for _, row in df_limites_faltantes.iterrows():\n",
    "        latitude = row['new_latitude']\n",
    "        longitude = row['new_longitude']\n",
    "        # Adiciona um item ao batch usando o formato específico da query\n",
    "        batch_items.append({\n",
    "            \"query\": f\"?query={latitude},{longitude}&returnSpeedLimit=true\"\n",
    "        })\n",
    "\n",
    "    # Monta o payload final\n",
    "    payload = {\n",
    "        \"batchItems\": batch_items\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'  # Importante: definir corretamente o Content-Type\n",
    "    }\n",
    "\n",
    "\n",
    "    # Fazendo a requisição para criar o job\n",
    "    response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "    # Verificando o resultado da criação do job\n",
    "    if response.status_code == 202:\n",
    "        # O job foi aceito e a URL para verificar o status foi retornada\n",
    "        operation_location = response.headers['location']\n",
    "        print(f\"Job criado com sucesso! Verifique o status em: {operation_location}\")\n",
    "\n",
    "        result_response = requests.get(operation_location)\n",
    "        result_data = result_response.json()\n",
    "\n",
    "        # Lista para armazenar os dados extraídos\n",
    "        data = []\n",
    "\n",
    "        # Extraindo os dados do JSON\n",
    "        for item in result_data['batchItems']:\n",
    "            address_info = item.get('response', {}).get('addresses', [{}])[0].get('address', {})\n",
    "            street_name = address_info.get('street', 'Desconhecido')\n",
    "            speed_limit = address_info.get('speedLimit', 'Sem velocidade')\n",
    "            data.append({'Street': street_name, 'Speed Limit': speed_limit})\n",
    "\n",
    "        # Criando um DataFrame a partir dos dados extraídos\n",
    "        df_limites = pd.DataFrame(data)\n",
    "\n",
    "        # Combinando os DataFrames\n",
    "        df_limites_faltantes = df_limites_faltantes.merge(df_limites, left_index=True, right_index=True)\n",
    "\n",
    "    else:\n",
    "        print(f\"Erro ao criar o job: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depois de encontrar os limites faltantes, precisa salvar no parquet.\n",
    "- Observação: podem haver casos que não encontrou um limite para o placeId. Mesmo nesse caso, o placeId é salvo com limite 0, para evitar redundancia e um gasto maior da API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(df_limites_faltantes) > 0):\n",
    "    #Captura somente o valor numerico do limite de velocidade e converte para float\n",
    "    df_limites_faltantes['limite_velocidade'] = df_limites_faltantes['Speed Limit'].str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    #Adicionar limite de velocidade como 0 nos casos em que não foi possível encontrar\n",
    "    df_limites_faltantes.loc[df_limites_faltantes['limite_velocidade'].isna(), 'limite_velocidade'] = 0\n",
    "\n",
    "    #Passar os novos limites para o parquet - Os placeId serao unicos???\n",
    "    df_limites_cache = pd.concat([df_limites_cache, df_limites_faltantes[['placeId', 'Street','limite_velocidade']]], ignore_index=True)\n",
    "    df_limites_cache.to_parquet('limites_velocidade.parquet')\n",
    "\n",
    "#Adicionar coluna de limite de velocidade\n",
    "df_processado = df_processado.merge(df_limites_cache, on='placeId', how='left')\n",
    "\n",
    "#Retira da analise os casos em que a velocidade é 0 ou o limite de velocidade é 0. Pois podem representar tanto uma situação de um veiculo parado, como uma inconsistencia na base de dados.\n",
    "#df_processado = df_processado[(df_processado['velocidade'] > 0) & (df_processado['limite_velocidade'] != 0)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critérios - Aceleração e Desaceleração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processado = df_processado.set_index('dataColetaDados')\n",
    "\n",
    "#Precisa converter a velocidade para m/s, pois o limite de velocidade vem em km/h\n",
    "df_processado['velocidade_convertida'] = df_processado['velocidade'] / 3.6\n",
    "df_processado['diff_tempo'] = df_processado.index.to_series().diff().dt.total_seconds()\n",
    "df_processado['diff_velocidade'] = df_processado['velocidade_convertida'].diff()  \n",
    "\n",
    "df_processado['aceleracao_derivada'] = df_processado['diff_velocidade'] / df_processado['diff_tempo']\n",
    "\n",
    "#Inconsistencia de corridas que os dados deram muitos alto, ou muito baixo, uma situação irreal\n",
    "df_processado = df_processado[(df_processado['aceleracao_derivada'] > -10) & (df_processado['aceleracao_derivada'] < 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráficos - Visulização dos Critérios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lendo tabela que já foi processada - OPCIONAL\n",
    "df_processado_normal = pd.read_parquet(r\"C:\\Users\\vidal\\Downloads\\normal.parquet\")\n",
    "df_processado_normal = df_processado_normal[df_processado_normal['limite_velocidade'] != 0]\n",
    "\n",
    "df_processado = pd.read_parquet(r\"C:\\Users\\vidal\\Downloads\\agressivo.parquet\")\n",
    "df_processado = df_processado[df_processado['limite_velocidade'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Velocidade x Limite de velocidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processado_grafico = df_processado.copy()\n",
    "\n",
    "# Criar máscara para áreas abaixo e acima do limite de velocidade\n",
    "abaixo_limite = df_processado_grafico['velocidade'] <= df_processado_grafico['limite_velocidade']\n",
    "acima_limite = df_processado_grafico['velocidade'] > df_processado_grafico['limite_velocidade']\n",
    "\n",
    "# Plotar o gráfico\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plotar a velocidade ao longo do tempo\n",
    "plt.plot(df_processado_grafico.index, df_processado_grafico['velocidade'], label='Velocidade', linewidth=2, color='darkblue')\n",
    "plt.plot(df_processado_grafico.index, df_processado_grafico['limite_velocidade'], label='Limite de Velocidade', linewidth=2, color='gray', linestyle='--')\n",
    "\n",
    "# Preencher áreas abaixo do limite (azul claro)\n",
    "plt.fill_between(\n",
    "    df_processado_grafico.index,\n",
    "    df_processado_grafico['velocidade'],\n",
    "    df_processado_grafico['limite_velocidade'],\n",
    "    where=abaixo_limite,\n",
    "    interpolate=True,\n",
    "    color='blue',\n",
    "    alpha=0.2,\n",
    "    label='Abaixo do Limite'\n",
    ")\n",
    "\n",
    "# Preencher áreas acima do limite (vermelho claro)\n",
    "plt.fill_between(\n",
    "    df_processado_grafico.index,\n",
    "    df_processado_grafico['velocidade'],\n",
    "    df_processado_grafico['limite_velocidade'],\n",
    "    where=acima_limite,\n",
    "    interpolate=True,\n",
    "    color='red',\n",
    "    alpha=0.2,\n",
    "    label='Acima do Limite'\n",
    ")\n",
    "\n",
    "# Estilo do gráfico\n",
    "plt.title('Critério Velocidade x Limite de Velocidade', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Tempo', fontsize=12)\n",
    "plt.ylabel('Velocidade (km/h)', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend(fontsize=10, loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aceleração e Desaceleração "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filtrar apenas as corridas que tenham pelo menos 10 minutos de duração\n",
    "df_selecionadas = df_processado.groupby('idCorrida').filter(lambda x: (x.index[-1] - x.index[0]).total_seconds() >= 600)\n",
    "\n",
    "# Parâmetros\n",
    "amostras_por_bloco = 500  # Número de pontos uniformes para reamostragem\n",
    "\n",
    "# Função para reamostrar os dados para um número fixo de pontos\n",
    "def reamostrar_bloco(bloco_df, n_pontos):\n",
    "    tempo_normalizado = np.linspace(0, 1, len(bloco_df))  # Normalizar o tempo\n",
    "    indices_interpolados = np.linspace(0, 1, n_pontos)    # Pontos interpolados uniformes\n",
    "    valores_interpolados = np.interp(indices_interpolados, tempo_normalizado, bloco_df['aceleracao_derivada'].values)\n",
    "    return valores_interpolados\n",
    "\n",
    "# Preparar os dados alinhados\n",
    "dados_grafico = []\n",
    "\n",
    "# Encontrar o tempo total das corridas selecionadas\n",
    "tempo_maximo = 0\n",
    "\n",
    "for corrida_id, corrida_df in df_selecionadas.groupby('idCorrida'):\n",
    "    corrida_df = corrida_df.sort_index()\n",
    "    inicio_corrida = corrida_df.index[0]\n",
    "    fim_corrida = corrida_df.index[-1]\n",
    "    \n",
    "    duracao_corrida = (fim_corrida - inicio_corrida).total_seconds() / 60  # Converter para minutos\n",
    "    tempo_maximo = max(tempo_maximo, duracao_corrida)\n",
    "\n",
    "    # Reamostrar toda a corrida em um único bloco\n",
    "    bloco_reamostrado = reamostrar_bloco(corrida_df, amostras_por_bloco)\n",
    "    dados_grafico.append(bloco_reamostrado)\n",
    "\n",
    "# Converter os dados em uma matriz\n",
    "dados_grafico_array = np.array(dados_grafico)\n",
    "\n",
    "# Calcular a mediana (antes era a média)\n",
    "mediana = np.median(dados_grafico_array, axis=0)\n",
    "\n",
    "# Gerar eixo X ajustado ao tempo total da corrida\n",
    "tempo_em_minutos = np.linspace(0, tempo_maximo, amostras_por_bloco)\n",
    "\n",
    "# Criar o gráfico atualizado sem percentis, mantendo apenas a mediana\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(tempo_em_minutos, mediana, label='Mediana', color='darkblue', linewidth=2)\n",
    "\n",
    "# Adicionar linhas tracejadas para aceleração agressiva e desaceleração agressiva\n",
    "plt.axhline(2.0, color='red', linestyle='--', linewidth=1.5, label='Aceleração Agressiva (2.0 m/s²)')\n",
    "plt.axhline(-3.5, color='red', linestyle='--', linewidth=1.5, label='Desaceleração Agressiva (-3.5 m/s²)')\n",
    "\n",
    "# Personalizar o gráfico\n",
    "plt.title('Padrões de Aceleração Derivada', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Tempo (minutos)', fontsize=12)\n",
    "plt.ylabel('Aceleração Derivada', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1)  # Linha de referência\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação - Segmentação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilizando uma lógica semelhante ao artigo. Calcula quantas vezes um critério de gravidade acontece em X metros, tanto para uma pessoa comum, quanto para uma pessoa agressiva. A partir disso classifica.\n",
    "\n",
    "Consultar: https://onlinelibrary.wiley.com/doi/10.1155/2018/9530470. Tem uma ampla gama de critérios\n",
    "\n",
    "O bloco abaixo busca calcular distancia entre lat e log, utilizando uma função chamada Haversine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Cópia do dataframe (ajuste conforme o necessário)\n",
    "df_classificacao_eventos = df_processado[['velocidade', 'rpm', 'temperatura', 'pressao', 'engineload', 'throttlePosition', 'limite_velocidade', 'aceleracao_derivada', \\\n",
    "                                           'new_latitude', 'new_longitude', 'Street', 'idCorrida', 'giroscopioYaw', 'giroscopioPitch', 'giroscopioRow', 'acelerometroEixoX', \\\n",
    "                                            'acelerometroEixoY', 'acelerometroEixoZ' ]].copy()\n",
    "\n",
    "\n",
    "# Função de Haversine - calcula a distância entre dois pontos geográficos\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Conversão de graus para radianos\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Diferenças\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    # Fórmula de Haversine\n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    # Raio médio da Terra (em km)\n",
    "    R = 6371.0\n",
    "    return R * c  # Distância em quilômetros\n",
    "\n",
    "# Adiciona uma coluna de distância entre pontos consecutivos\n",
    "df_classificacao_eventos[\"distance\"] = np.where(\n",
    "    df_classificacao_eventos[\"idCorrida\"] != df_classificacao_eventos[\"idCorrida\"].shift(), \n",
    "    0,  # Caso a corrida mude, a distância é 0\n",
    "    haversine(\n",
    "        df_classificacao_eventos[\"new_latitude\"].shift(), \n",
    "        df_classificacao_eventos[\"new_longitude\"].shift(), \n",
    "        df_classificacao_eventos[\"new_latitude\"], \n",
    "        df_classificacao_eventos[\"new_longitude\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Preenche valores NaN com 0 (para a primeira linha)\n",
    "df_classificacao_eventos[\"distance\"].fillna(0, inplace=True)\n",
    "\n",
    "# Calcula a distância total por corrida (opcional)\n",
    "distancias_por_corrida = df_classificacao_eventos.groupby(\"idCorrida\")[\"distance\"].sum()\n",
    "\n",
    "# Calcula a distância total\n",
    "total_distance = df_classificacao_eventos[\"distance\"].sum()\n",
    "\n",
    "print(f\"Distância total percorrida: {total_distance:.2f} km\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gera a quantidade de vezes que ocorreu um evento. No fim a classificação é geral, levando em consideração a quantidade total de km percorridos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objetivo é calcular a quantidade de vezes que um critério ocorre.\n",
    "#Critérios:\n",
    "#ALV1 -> Até 20% acima do limite de velocidade. Existe uma tolerencia de 7 km quando limite é de até 100km/h. Após isso, a tolerancia é de 7%\n",
    "#ALV2 -> 20% acima, até 50% acima do limite de velocidade\n",
    "#ALV3 -> Mais de 50% acima do limite de velocidade\n",
    "#AA -> Aceleração acima de 3 m/s²\n",
    "#DA -> Desaceleração acima de 4 m/s²\n",
    "\n",
    "#df_classificacao_eventos = df_classificacao_eventos[df_classificacao_eventos['limite_velocidade'] != 0]\n",
    "\n",
    "# Calcular os critérios sem utilizar iterrows\n",
    "df_classificacao_eventos['tolerancia'] = np.where(\n",
    "    df_classificacao_eventos['limite_velocidade'] <= 100,\n",
    "    7,\n",
    "    df_classificacao_eventos['limite_velocidade'] * 0.07\n",
    ")\n",
    "\n",
    "# Critérios de classificação considerando a tolerância\n",
    "df_classificacao_eventos['ALV1'] = (\n",
    "    (df_classificacao_eventos['velocidade'] > (df_classificacao_eventos['limite_velocidade'] + df_classificacao_eventos['tolerancia'])) &\n",
    "    (df_classificacao_eventos['velocidade'] <= (df_classificacao_eventos['limite_velocidade'] * 1.2 + df_classificacao_eventos['tolerancia']))\n",
    ").astype(int)\n",
    "\n",
    "df_classificacao_eventos['ALV2'] = (\n",
    "    (df_classificacao_eventos['velocidade'] > (df_classificacao_eventos['limite_velocidade'] * 1.2 + df_classificacao_eventos['tolerancia'])) &\n",
    "    (df_classificacao_eventos['velocidade'] <= (df_classificacao_eventos['limite_velocidade'] * 1.5 + df_classificacao_eventos['tolerancia']))\n",
    ").astype(int)\n",
    "\n",
    "df_classificacao_eventos['ALV3'] = (\n",
    "    df_classificacao_eventos['velocidade'] > (df_classificacao_eventos['limite_velocidade'] * 1.5 + df_classificacao_eventos['tolerancia'])\n",
    ").astype(int)\n",
    "\n",
    "df_classificacao_eventos['AA'] = (df_classificacao_eventos['aceleracao_derivada'] >= 2.0).astype(int)\n",
    "\n",
    "df_classificacao_eventos['DA'] = (df_classificacao_eventos['aceleracao_derivada'] <= -3.5).astype(int)\n",
    "\n",
    "\n",
    "print(f'Dentro de {total_distance:.2f} km, ocorreram as seguintes quantidades de eventos:')\n",
    "df_eventos = df_classificacao_eventos[['ALV1', 'ALV2', 'ALV3', 'AA', 'DA']].sum().to_frame().transpose()\n",
    "df_eventos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A partir da classificação geral, busca quebrar em trechos de X metros, existe um problema de trechos maiores do que X metros, ou seja, gera menos trechos do que a quantidade total percorrida. Exemplo: Uma base com uma distancia de 50 km, pode ter somente 43 trechos. No fim os trechos não tem necessariamente a distancia esperada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classificacao_eventos['distancia_acumulada'] = df_classificacao_eventos['distance'].cumsum()\n",
    "df_classificacao_eventos['segmento'] = (df_classificacao_eventos['distancia_acumulada'] // 0.1).astype(int) #100 metros\n",
    "\n",
    "df_classificacao_eventos['latitude_inicial'] = df_classificacao_eventos.groupby('segmento')['new_latitude'].transform('first')\n",
    "df_classificacao_eventos['longitude_inicial'] = df_classificacao_eventos.groupby('segmento')['new_longitude'].transform('first')\n",
    "df_classificacao_eventos['latitude_final'] = df_classificacao_eventos.groupby('segmento')['new_latitude'].transform('last')\n",
    "df_classificacao_eventos['longitude_final'] = df_classificacao_eventos.groupby('segmento')['new_longitude'].transform('last')\n",
    "df_classificacao_eventos['soma_evento_segmento_aa'] = df_classificacao_eventos.groupby('segmento')['AA'].transform('sum')\n",
    "df_classificacao_eventos['soma_evento_segmento_da'] = df_classificacao_eventos.groupby('segmento')['DA'].transform('sum')\n",
    "df_classificacao_eventos['soma_evento_segmento_alv1'] = df_classificacao_eventos.groupby('segmento')['ALV1'].transform('sum')\n",
    "df_classificacao_eventos['soma_evento_segmento_alv2'] = df_classificacao_eventos.groupby('segmento')['ALV2'].transform('sum')\n",
    "df_classificacao_eventos['soma_evento_segmento_alv3'] = df_classificacao_eventos.groupby('segmento')['ALV3'].transform('sum')\n",
    "\n",
    "\n",
    "#Esse df é só para visualizar os casos\n",
    "df_segmentos = df_classificacao_eventos.groupby('segmento').agg({'AA': 'sum', 'DA': 'sum', \\\n",
    "                                                                    'ALV1': 'sum', 'ALV2': 'sum',\n",
    "                                                                      'ALV3':  'sum', 'latitude_inicial': 'first', \\\n",
    "                                                                'longitude_inicial': 'first', 'latitude_final' : 'first', \n",
    "                                                                'longitude_final': 'first'  }).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "df_segmentos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Faz a classificação de cada segmento. Considerando somente dois grupos: Normal e Agressivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classificacao_eventos['total_eventos'] = df_classificacao_eventos['soma_evento_segmento_aa'] + df_classificacao_eventos['soma_evento_segmento_da'] + \\\n",
    "    df_classificacao_eventos['soma_evento_segmento_alv1'] + df_classificacao_eventos['soma_evento_segmento_alv2'] + df_classificacao_eventos['soma_evento_segmento_alv3']\n",
    "\n",
    "df_classificacao_eventos.loc[df_classificacao_eventos['total_eventos'] >= 1, 'classificacao'] = 'AGRESSIVO'\n",
    "df_classificacao_eventos.loc[df_classificacao_eventos['total_eventos'] == 0, 'classificacao'] = 'NORMAL'\n",
    "\n",
    "df_classificacao_eventos[['segmento','ALV1', 'ALV2', 'ALV3', 'AA', 'DA', 'classificacao']].groupby('segmento').agg({'ALV1': 'sum', 'ALV2': 'sum', 'ALV3': 'sum', 'AA': 'sum', 'DA': 'sum', 'classificacao': 'first'}).head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gera diferentes formas de classificação. Sempre considera a classificação levando como base o segmento de 100 metros, mas com ideias de classificação diferentes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nesse bloco são feitos diferentes tipos de classificações.\n",
    "# 1 - Classificação por segmento mantendo linha a linha. \n",
    "# 2 - Classificação por segmento, mas mantendo somente um registro por segmento\n",
    "# 3 - Classificação considerando a moda do que mais acontece para o segmento. Mantem somente um registro por segmento\n",
    "\n",
    "df_classificacao_eventos = df_classificacao_eventos.reset_index()\n",
    "\n",
    "df_classificacao_eventos['motorista'] = 'eduardo'\n",
    "\n",
    "df_final = pd.read_excel(\"df_final.xlsx\")\n",
    "df_final = pd.concat([df_final, df_classificacao_eventos], ignore_index=True)\n",
    "df_final.to_excel(\"df_final.xlsx\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "#df_classificacao_eventos_segmentos = df_classificacao_eventos.copy()\n",
    "#df_classificacao_eventos_segmentos = df_classificacao_eventos_segmentos.groupby('segmento').agg({'velocidade': 'mean', 'rpm': 'mean', 'temperatura': 'mean', 'pressao': 'mean', 'engineload': 'mean', 'throttlePosition': 'mean', 'limite_velocidade': 'mean', 'aceleracao_derivada': 'mean', \\\n",
    "#                                           'new_latitude': 'first', 'new_longitude': 'first', 'Street': 'first', 'giroscopioYaw': 'mean', 'giroscopioPitch': 'mean', 'giroscopioRow': 'mean', 'acelerometroEixoX': 'mean', \\\n",
    "#                                           'acelerometroEixoY': 'mean', 'acelerometroEixoZ': 'mean', 'ALV1': 'sum', 'ALV2': 'sum', 'ALV3': 'sum', 'AA': 'sum', 'DA': 'sum', 'classificacao': 'first', 'motorista': 'first'}).reset_index()\n",
    "#df_classificacao_eventos_segmentos = df_classificacao_eventos_segmentos.drop(columns=['segmento'])\n",
    "\n",
    "\n",
    "#df_final_seg = pd.read_excel(\"df_ml_segmentos_validacao.xlsx\")\n",
    "#df_final_seg = pd.concat([df_final_seg, df_classificacao_eventos_segmentos], ignore_index=True)\n",
    "#df_final_seg.to_excel(\"df_ml_segmentos_validacao.xlsx\", index=False)\n",
    "\n",
    "#df_classificacao_moda['total_eventos'] = df_classificacao_moda['ALV1'] + df_classificacao_moda['ALV2'] + df_classificacao_moda['ALV3'] + df_classificacao_moda['AA'] + df_classificacao_moda['DA']\n",
    "#df_classificacao_moda.loc[df_classificacao_moda['total_eventos'] >= 1, 'classificacao'] = 'AGRESSIVO'\n",
    "#df_classificacao_moda.loc[df_classificacao_moda['total_eventos'] == 0, 'classificacao'] = 'NORMAL'\n",
    "\n",
    "#df_classificacao_moda= df_classificacao_moda.groupby('segmento').agg({\n",
    "#    'velocidade': 'mean',\n",
    "#    'rpm': 'mean',\n",
    "#    'temperatura': 'mean',\n",
    "#    'pressao': 'mean',\n",
    "#    'engineload': 'mean',\n",
    "#    'throttlePosition': 'mean',\n",
    "#    'limite_velocidade': 'mean',\n",
    "#    'aceleracao_derivada': 'mean',\n",
    "#    'giroscopioYaw': 'mean',\n",
    "#    'giroscopioPitch': 'mean',\n",
    "#    'giroscopioRow': 'mean',\n",
    "#    'acelerometroEixoX': 'mean',\n",
    "#    'acelerometroEixoY': 'mean',\n",
    "#    'acelerometroEixoZ': 'mean',\n",
    "#    'ALV1': 'sum',\n",
    "#    'ALV2': 'sum',\n",
    "#    'ALV3': 'sum',\n",
    "#    'AA': 'sum',\n",
    "#    'DA': 'sum',\n",
    "#    'classificacao': lambda x: x.mode()[0],\n",
    "#    'motorista': 'first'\n",
    "#}).reset_index()\n",
    "\n",
    "#df_classificacao_moda = df_classificacao_moda.drop(columns=['segmento'])\n",
    "\n",
    "#df_final_moda = pd.read_excel(\"df_ml_moda_validacao.xlsx\")\n",
    "#df_final_moda = pd.concat([df_final_moda, df_classificacao_moda], ignore_index=True)\n",
    "#f_final_moda.to_excel(\"df_ml_moda_validacao.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning - Validação\n",
    "\n",
    "- Base Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ml = pd.read_excel(r\"C:\\Users\\vidal\\Downloads\\TCC\\ClassificacaoCondutores\\df_final.xlsx\")\n",
    "\n",
    "df_ml = df_ml[[ 'velocidade', 'rpm', 'temperatura', 'pressao', 'engineload', 'throttlePosition', 'limite_velocidade', 'aceleracao_derivada',  'classificacao']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_scaled = scaler.fit_transform(df_ml.drop(columns=['classificacao']))\n",
    "\n",
    "df_ml_scaled = pd.DataFrame(x_scaled, columns=df_ml.drop(columns=['classificacao']).columns)\n",
    "\n",
    "df_ml_scaled['classificacao'] = df_ml['classificacao']\n",
    "\n",
    "df_ml_scaled.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Teste de correlação entre variaveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_ml_scaled['classificacao'] = le.fit_transform(df_ml_scaled['classificacao'])\n",
    "\n",
    "# Calcular a matriz de correlação\n",
    "correlation_matrix = df_ml_scaled.corr()\n",
    "\n",
    "# Configurar o tamanho da figura\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Criar um mapa de calor da matriz de correlação\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "\n",
    "# Configurar o título do gráfico\n",
    "plt.title('Matriz de Correlação das Variáveis', fontsize=16)\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Separar as features e o target\n",
    "X_train = df_ml_scaled.drop(columns=['classificacao'])\n",
    "y_train = df_ml_scaled['classificacao']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=42, stratify=y_train)\n",
    "\n",
    "# Construir e treinar o modelo Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões e avaliar o modelo\n",
    "y_pred_random = rf_model.predict(X_test)\n",
    "print(\"Acurácia:\", accuracy_score(y_test, y_pred_random))\n",
    "print(classification_report(y_test, y_pred_random, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressão Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Separar as features e o target\n",
    "X_train = df_ml_scaled.drop(columns=['classificacao'])\n",
    "y_train = df_ml_scaled['classificacao']\n",
    "\n",
    "# Dividir o conjunto de dados para treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=42, stratify=y_train)\n",
    "\n",
    "# Construir e treinar o modelo de Regressão Logística\n",
    "rl_model = LogisticRegression(random_state=42)\n",
    "rl_model.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões e avaliar o modelo\n",
    "y_pred_rl = rl_model.predict(X_test)\n",
    "print(\"Acurácia:\", accuracy_score(y_test, y_pred_rl))\n",
    "print(classification_report(y_test, y_pred_rl, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Criar e treinar o modelo SVM\n",
    "svm_model = SVC(kernel=\"rbf\", C=10.0, decision_function_shape='ovo', class_weight='balanced')  # Testar 'linear' ou 'rbf'\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(\"Acurácia:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados - Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validação - Matriz de convulsão\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "cf_matrix_rf = confusion_matrix(y_test, y_pred_random)\n",
    "cf_matrix_rl = confusion_matrix(y_test, y_pred_rl)\n",
    "cf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.heatmap(cf_matrix_rf, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.title('Matriz de Confusão - Random Forest')\n",
    "plt.xlabel('Previsto')\n",
    "plt.ylabel('Real')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.heatmap(cf_matrix_rl, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.title('Matriz de Confusão - Regressão Logística')\n",
    "plt.xlabel('Previsto')\n",
    "plt.ylabel('Real')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.heatmap(cf_matrix_svm, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.title('Matriz de Confusão - SVM')\n",
    "plt.xlabel('Previsto')\n",
    "plt.ylabel('Real')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapa com o circuito classificado\n",
    "- Gerando um mapa com os segmentos no folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processado_agressivo = pd.read_excel(r\"C:\\Users\\vidal\\Downloads\\TCC\\ClassificacaoCondutores\\df_final.xlsx\")\n",
    "\n",
    "df_processado_agressivo = df_processado_agressivo[(df_processado_agressivo['motorista'] ==  'celso') & (df_processado_agressivo['idCorrida'] == 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_processado_agressivo = df_processado_agressivo[df_processado_agressivo['limite_velocidade'] != 0]\n",
    "\n",
    "# Seleciona as colunas usadas no treinamento\n",
    "feature_cols = ['velocidade', 'rpm', 'temperatura', 'pressao', \n",
    "                'engineload', 'throttlePosition', 'limite_velocidade', 'aceleracao_derivada']\n",
    "\n",
    "# Prever a classificação para o dataset \"agressivo\"\n",
    "df_agressivo_ml = df_processado_agressivo.copy()\n",
    "X_agressivo = df_agressivo_ml[feature_cols]\n",
    "X_agressivo_scaled = scaler.transform(X_agressivo)\n",
    "df_agressivo_ml['classificacao'] = svm_model.predict(X_agressivo_scaled)\n",
    "\n",
    "# Combina os datasets classificados (mantendo as coordenadas) para uso no mapa\n",
    "df_geracao = df_agressivo_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Cria o mapa centralizado na primeira coordenada\n",
    "m = folium.Map(location=[df_geracao['new_latitude'].iloc[0], \n",
    "                         df_geracao['new_longitude'].iloc[0]], \n",
    "               zoom_start=15)\n",
    "\n",
    "# Adiciona os pontos e linhas ao mapa\n",
    "for i in range(len(df_geracao) - 1):\n",
    "    # Coordenadas do ponto atual e do próximo ponto\n",
    "    lat1, lon1 = df_geracao.iloc[i][['new_latitude', 'new_longitude']]\n",
    "    lat2, lon2 = df_geracao.iloc[i + 1][['new_latitude', 'new_longitude']]\n",
    "    \n",
    "    # Classificação do ponto atual\n",
    "    classificacao = df_geracao.iloc[i]['classificacao']\n",
    "    \n",
    "    # Define a cor da linha com base na classificação\n",
    "    line_color = 'red' if classificacao == 0 else 'green'\n",
    "    \n",
    "    # Adiciona uma linha entre o ponto atual e o próximo\n",
    "    folium.PolyLine([(lat1, lon1), (lat2, lon2)], color=line_color, weight=5).add_to(m)\n",
    "    \n",
    "    # Adiciona um marcador no ponto atual\n",
    "    #folium.Marker(\n",
    "    #    location=[lat1, lon1],\n",
    "    #    popup=f\"Coordenadas: ({lat1}, {lon1})<br>Classificação: {classificacao}\",\n",
    "    #   icon=folium.Icon(color='blue', icon='info-sign')\n",
    "    #).add_to(m)\n",
    "\n",
    "# Salva o mapa em um arquivo HTML e exibe o mapa\n",
    "output_file = r\"C:\\Users\\vidal\\Downloads\\TCC\\ClassificacaoCondutores\\percurso.html\"\n",
    "m.save(output_file)\n",
    "output_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
