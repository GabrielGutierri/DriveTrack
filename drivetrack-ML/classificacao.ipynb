{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e8bb92",
   "metadata": {},
   "source": [
    "## Leitura de corrida realizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lê todos os dados que existem para uma entidade, e junta todos os atributos em um dataframe. Preciso modificar a entidade na mão\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "\n",
    "    \n",
    "def fetch_paginated_data(url_base, attribute_name, h_limit=100, is_location=False):\n",
    "    \"\"\"\n",
    "    Busca dados usando paginação com hLimit e hOffset.\n",
    "    \"\"\"\n",
    "    dict_data = {attribute_name: [], 'recvTime_' + attribute_name: []}\n",
    "    h_offset = 0\n",
    "\n",
    "    while True:\n",
    "        # Construir URL com hLimit e hOffset\n",
    "        url = f\"{url_base}?hLimit={h_limit}&hOffset={h_offset}\"\n",
    "        headers = {\n",
    "            'fiware-service': 'smart',\n",
    "            'fiware-servicepath': '/'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            values = data['contextResponses'][0]['contextElement']['attributes'][0]['values']\n",
    "            if not values:\n",
    "                break  # Interrompe se não houver mais valores\n",
    "\n",
    "            for value in values:\n",
    "                dict_data[attribute_name].append(\n",
    "                    value['attrValue']['coordinates'] if is_location else value['attrValue']\n",
    "                )\n",
    "                dict_data['recvTime_' + attribute_name].append(value['recvTime'])\n",
    "\n",
    "            # Incrementa o offset para a próxima página\n",
    "            h_offset += h_limit\n",
    "        else:\n",
    "            print(f\"Erro na requisição: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(dict_data)\n",
    "\n",
    "\n",
    "# Definir atributos e URLs\n",
    "attributes = [\n",
    "    (\"velocidade\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:grandSiena:ewl6143/attributes/velocidade\"),\n",
    "    (\"rpm\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:grandSiena:ewl6143/attributes/rpm\"),\n",
    "    (\"temperatura\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:grandSiena:ewl6143/attributes/temperature\"),\n",
    "    (\"pressao\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:grandSiena:ewl6143/attributes/pressure\"),\n",
    "    (\"dataColetaDados\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:grandSiena:ewl6143/attributes/dataColetaDados\"),\n",
    "    (\"location\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:grandSiena:ewl6143/attributes/location\"),\n",
    "    (\"engineload\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:grandSiena:ewl6143/attributes/engineload\"),\n",
    "    (\"idCorrida\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:grandSiena:ewl6143/attributes/idCorrida\"),\n",
    "    (\"throttlePosition\", \"http://20.62.95.153:8666/STH/v1/contextEntities/type/iot/id/urn:ngsi-ld:grandSiena:ewl6143/attributes/throttlePosition\"),\n",
    "]\n",
    "\n",
    "# DataFrames para armazenar os resultados\n",
    "dfs = {attribute: pd.DataFrame() for attribute, _ in attributes}\n",
    "\n",
    "# Requisições paralelas para todos os atributos\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future_to_attribute = {\n",
    "        executor.submit(fetch_paginated_data, url, attribute, 100, attribute == \"location\"): attribute\n",
    "        for attribute, url in attributes\n",
    "    }\n",
    "    for future in future_to_attribute:\n",
    "        attribute = future_to_attribute[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            dfs[attribute] = result\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao buscar {attribute}: {e}\")\n",
    "\n",
    "# Combinar os DataFrames baseando-se nos timestamps\n",
    "df = dfs[\"velocidade\"]\n",
    "for attribute in [attr for attr, _ in attributes if attr != \"velocidade\"]:\n",
    "    df = df.merge(\n",
    "        dfs[attribute],\n",
    "        how=\"outer\",\n",
    "        left_on=\"recvTime_velocidade\",\n",
    "        right_on=f\"recvTime_{attribute}\"\n",
    "    )\n",
    "\n",
    "# Limpar o DataFrame final\n",
    "\n",
    "df = df.drop(columns=['recvTime_velocidade', 'recvTime_rpm', 'recvTime_temperatura', 'recvTime_pressao', 'recvTime_dataColetaDados',  'recvTime_location', 'recvTime_engineload', 'recvTime_idCorrida', 'recvTime_throttlePosition'])\n",
    "df = df.dropna()\n",
    "\n",
    "# Exibir o resultado\n",
    "df_raw = df.copy()\n",
    "\n",
    "df_raw\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb42dab9",
   "metadata": {},
   "source": [
    "## Tratamento de dados \n",
    "\n",
    "A partir do dataset coletado, é preciso fazer algumas correções. \n",
    "\n",
    "- Converter as colunas para cada tipo\n",
    "- Retirar casos inconsistentes. Exemplo: data = 0, location[0,0], dados com -999\n",
    "- Encontrar as corridas que existem nas entidades\n",
    "- Separar o location em lat e log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "df = df_raw\n",
    "\n",
    "#Conversão do tipo de dado.\n",
    "columns = {\n",
    "    'idCorrida': int,\n",
    "    'velocidade': int,\n",
    "    'rpm': float,\n",
    "    'temperatura': int,\n",
    "    'pressao': int,\n",
    "    'engineload': float,\n",
    "    'throttlePosition': float,\n",
    "    'dataColetaDados': datetime,\n",
    "    'location': str,\n",
    "}\n",
    "\n",
    "#Retirar casos em que o dia veio como 0. É uma inconsistencia geral\n",
    "df = df[df['dataColetaDados'] != '0']\n",
    "\n",
    "\n",
    "for column, dtype in columns.items():\n",
    "    if dtype == datetime:\n",
    "        df[column] = pd.to_datetime(df[column])\n",
    "    else:\n",
    "        df[column] = df[column].astype(dtype)\n",
    "\n",
    "df = df.set_index('dataColetaDados')\n",
    "\n",
    "# Remover valores nulos ou inconsistentes em 'location'\n",
    "df = df[df['location'].notna() & df['location'].str.startswith('[')]\n",
    "\n",
    "# Separar latitude e longitude usando split\n",
    "df['latitude'] = df['location'].apply(lambda x: x.strip('[]').split(', ')[0]).astype(float)\n",
    "df['longitude'] = df['location'].apply(lambda x: x.strip('[]').split(', ')[1]).astype(float)\n",
    "\n",
    "#Inconsistência de longitude e latitude serem 0\n",
    "df = df[(df['latitude'] != 0) & (df['longitude'] != 0)]\n",
    "\n",
    "#Removendo valores extremos. Normalmente eles valem -999\n",
    "df = df[(df['velocidade'] != -999) & (df['rpm'] != -999) & (df['pressao'] != -999) & (df['temperatura'] != -999) & (df['engineload'] != -999) & (df['throttlePosition'] != -999)]\n",
    "\n",
    "#Considero um unico dataframe, pois o processo é mais linear\n",
    "df_processado = df[['velocidade', 'rpm', 'temperatura', 'pressao', 'engineload', 'throttlePosition', 'location', 'latitude', 'longitude', 'idCorrida']].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05598070",
   "metadata": {},
   "source": [
    "## Tratamento de Dados - Correção de coordenada\n",
    "- Correção de latitude e longitude usando API google Snap to Roads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010fadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar o índice em uma coluna e resetar o índice\n",
    "df_processado = df_processado.reset_index()\n",
    "\n",
    "# Renomear a coluna do índice, se necessário\n",
    "df_processado = df_processado.rename(columns={'index': 'novo_indice'})\n",
    "\n",
    "# Chave para acesso a API do Google Maps. Substitua pela sua chave\n",
    "api_key = \n",
    "\n",
    "# Número máximo de coordenadas por batch\n",
    "batch_size = 100\n",
    "new_coords = {}\n",
    "new_place_ids = {}\n",
    "\n",
    "# Iterar sobre o DataFrame em batches\n",
    "for start in range(0, len(df_processado), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch = df_processado.iloc[start:end]\n",
    "\n",
    "    # Criar o path para a requisição\n",
    "    path = '|'.join([f\"{lat},{lon}\" for lat, lon in zip(batch['latitude'], batch['longitude'])])\n",
    "\n",
    "    # Fazer a requisição para o Google Maps API\n",
    "    url = f\"https://roads.googleapis.com/v1/snapToRoads?path={path}&key={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    snapped_points = response.json()\n",
    "\n",
    "    # Extrair as novas coordenadas e placeIds, ajustando o índice para o DataFrame original\n",
    "    for point in snapped_points.get('snappedPoints', []):\n",
    "        relative_index = point['originalIndex']  # Índice dentro do batch (0-99)\n",
    "        original_index = relative_index + start  # Índice absoluto no DataFrame original\n",
    "        # Adicionar nova coordenada\n",
    "        new_coords[original_index] = (\n",
    "            point['location']['latitude'], \n",
    "            point['location']['longitude']\n",
    "        )\n",
    "        # Adicionar placeId, se disponível\n",
    "        if 'placeId' in point:\n",
    "            new_place_ids[original_index] = point['placeId']\n",
    "\n",
    "# Adicionar as novas coordenadas ao DataFrame original\n",
    "df_processado['new_latitude'] = df_processado.index.map(lambda idx: new_coords[idx][0] if idx in new_coords else None)\n",
    "df_processado['new_longitude'] = df_processado.index.map(lambda idx: new_coords[idx][1] if idx in new_coords else None)\n",
    "\n",
    "# Adicionar a coluna de placeId ao DataFrame original\n",
    "df_processado['placeId'] = df_processado.index.map(lambda idx: new_place_ids[idx] if idx in new_place_ids else None)\n",
    "\n",
    "# Exibir o DataFrame atualizado\n",
    "df_processado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab6a5e",
   "metadata": {},
   "source": [
    "## Critérios - Limite de Velocidade\n",
    "\n",
    "- Por conta do baixo limite de requisições gratuitas da API da Azure, é preciso reduzir as requisições. Os 3 blocos abaixo avaliam limites de velocidade já coletados, e só requisitão os novos casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0385131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linhas que não receberam novas coordenadas são removidas\n",
    "df_processado = df_processado.dropna(subset=['new_latitude', 'new_longitude'])\n",
    "\n",
    "#Parquet que armazena placeId e o limite de velocidade\n",
    "df_limites_cache = pd.read_parquet(\"limites_velocidade.parquet\")\n",
    "\n",
    "#Verificando se existem novos limites de velocidade a ser capturado\n",
    "df_limites_faltantes = df_processado.merge(df_limites_cache, on='placeId', how='left')\n",
    "df_limites_faltantes = df_limites_faltantes[df_limites_faltantes['limite_velocidade'].isna()]\n",
    "\n",
    "if(len(df_limites_faltantes) > 0):\n",
    "    \n",
    "    #Criar um novo parquet com os novos limites\n",
    "    df_limites_faltantes = df_limites_faltantes.drop_duplicates(subset=['placeId'], ignore_index=True)\n",
    "    print(len(df_limites_faltantes))\n",
    "    df_limites_faltantes = df_limites_faltantes.drop(columns=['limite_velocidade', 'Street'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283b3dcd",
   "metadata": {},
   "source": [
    "- Utilizando API Azure para encontrar limite da via ( Somente os casos que ainda não tem limite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e28ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(df_limites_faltantes) > 0):\n",
    "\n",
    "    import json\n",
    "\n",
    "    # Chave para acesso a API do Azure. Substitua pela sua chave\n",
    "    key = \n",
    "    \n",
    "    # Definindo o URL do endpoint batch\n",
    "    url = f\"https://atlas.microsoft.com/search/address/reverse/batch/json?api-version=1.0&subscription-key={key}\"\n",
    "\n",
    "    # Criando o payload dinamicamente a partir do DataFrame. Não precisa ter limite de 100 coordenadas para a API da Azure\n",
    "    batch_items = []\n",
    "\n",
    "    #Precisa resetar o index mais uma vez\n",
    "    # Transformar o índice em uma coluna e resetar o índice\n",
    "    #df_limites_faltantes = df_limites_faltantes.reset_index()\n",
    "\n",
    "    # Renomear a coluna do índice, se necessário\n",
    "    #df_limites_faltantes = df_limites_faltantes.rename(columns={'index': 'novo_indice'})\n",
    "\n",
    "    for _, row in df_limites_faltantes.iterrows():\n",
    "        latitude = row['new_latitude']\n",
    "        longitude = row['new_longitude']\n",
    "        # Adiciona um item ao batch usando o formato específico da query\n",
    "        batch_items.append({\n",
    "            \"query\": f\"?query={latitude},{longitude}&returnSpeedLimit=true\"\n",
    "        })\n",
    "\n",
    "    # Monta o payload final\n",
    "    payload = {\n",
    "        \"batchItems\": batch_items\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'  # Importante: definir corretamente o Content-Type\n",
    "    }\n",
    "\n",
    "\n",
    "    # Fazendo a requisição para criar o job\n",
    "    response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "    # Verificando o resultado da criação do job\n",
    "    if response.status_code == 202:\n",
    "        # O job foi aceito e a URL para verificar o status foi retornada\n",
    "        operation_location = response.headers['location']\n",
    "        print(f\"Job criado com sucesso! Verifique o status em: {operation_location}\")\n",
    "\n",
    "        result_response = requests.get(operation_location)\n",
    "        result_data = result_response.json()\n",
    "\n",
    "        # Lista para armazenar os dados extraídos\n",
    "        data = []\n",
    "\n",
    "        # Extraindo os dados do JSON\n",
    "        for item in result_data['batchItems']:\n",
    "            address_info = item.get('response', {}).get('addresses', [{}])[0].get('address', {})\n",
    "            street_name = address_info.get('street', 'Desconhecido')\n",
    "            speed_limit = address_info.get('speedLimit', 'Sem velocidade')\n",
    "            data.append({'Street': street_name, 'Speed Limit': speed_limit})\n",
    "\n",
    "        # Criando um DataFrame a partir dos dados extraídos\n",
    "        df_limites = pd.DataFrame(data)\n",
    "\n",
    "        # Combinando os DataFrames\n",
    "        df_limites_faltantes = df_limites_faltantes.merge(df_limites, left_index=True, right_index=True)\n",
    "\n",
    "    else:\n",
    "        print(f\"Erro ao criar o job: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba43e991",
   "metadata": {},
   "source": [
    "- Depois de encontrar os limites faltantes, precisa salvar no parquet.\n",
    "- Observação: podem haver casos que não encontrou um limite para o placeId. Mesmo nesse caso, o placeId é salvo com limite 0, para evitar redundancia e um gasto maior da API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23bf693",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(df_limites_faltantes) > 0):\n",
    "    #Captura somente o valor numerico do limite de velocidade e converte para float\n",
    "    df_limites_faltantes['limite_velocidade'] = df_limites_faltantes['Speed Limit'].str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    #Adicionar limite de velocidade como 0 nos casos em que não foi possível encontrar\n",
    "    df_limites_faltantes.loc[df_limites_faltantes['limite_velocidade'].isna(), 'limite_velocidade'] = 0\n",
    "\n",
    "    #Passar os novos limites para o parquet - Os placeId serao unicos???\n",
    "    df_limites_cache = pd.concat([df_limites_cache, df_limites_faltantes[['placeId', 'Street','limite_velocidade']]], ignore_index=True)\n",
    "    df_limites_cache.to_parquet('limites_velocidade.parquet')\n",
    "\n",
    "#Adicionar coluna de limite de velocidade\n",
    "df_processado = df_processado.merge(df_limites_cache, on='placeId', how='left')\n",
    "\n",
    "#Retira da analise os casos em que a velocidade é 0 ou o limite de velocidade é 0. Pois podem representar tanto uma situação de um veiculo parado, como uma inconsistencia na base de dados.\n",
    "#df_processado = df_processado[(df_processado['velocidade'] > 0) & (df_processado['limite_velocidade'] != 0)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadecb1f",
   "metadata": {},
   "source": [
    "Aceleração e Desaceleração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f6c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processado = df_processado.set_index('dataColetaDados')\n",
    "\n",
    "#Precisa converter a velocidade para m/s, pois o limite de velocidade vem em km/h\n",
    "df_processado['velocidade_convertida'] = df_processado['velocidade'] / 3.6\n",
    "df_processado['diff_tempo'] = df_processado.index.to_series().diff().dt.total_seconds()\n",
    "df_processado['diff_velocidade'] = df_processado['velocidade_convertida'].diff()  \n",
    "\n",
    "df_processado['aceleracao_derivada'] = df_processado['diff_velocidade'] / df_processado['diff_tempo']\n",
    "\n",
    "#Inconsistencia de corridas que os dados deram muitos alto, ou muito baixo, uma situação irreal\n",
    "df_processado = df_processado[(df_processado['aceleracao_derivada'] > -10) & (df_processado['aceleracao_derivada'] < 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5941ec",
   "metadata": {},
   "source": [
    "## Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "df_processado_agressivo = df_processado[df_processado['limite_velocidade'] != 0]\n",
    "\n",
    "# Seleciona as colunas usadas no treinamento\n",
    "feature_cols = ['velocidade', 'rpm', 'temperatura', 'pressao', \n",
    "                'engineload', 'throttlePosition', 'limite_velocidade', 'aceleracao_derivada']\n",
    "\n",
    "# Prever a classificação para o dataset \"agressivo\"\n",
    "df_agressivo_ml = df_processado_agressivo.copy()\n",
    "X_agressivo = df_agressivo_ml[feature_cols]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_agressivo)\n",
    "X_agressivo_scaled = scaler.transform(X_agressivo)\n",
    "\n",
    "# Prever a classificação usando o modelo SVM\n",
    "svm_model = joblib.load(\"svm_model.joblib\")\n",
    "df_agressivo_ml['classificacao'] = svm_model.predict(X_agressivo_scaled)\n",
    "\n",
    "# Combina os datasets classificados (mantendo as coordenadas) para uso no mapa\n",
    "df_geracao = df_agressivo_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b51916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Cria o mapa centralizado na primeira coordenada\n",
    "m = folium.Map(location=[df_geracao['new_latitude'].iloc[0], \n",
    "                         df_geracao['new_longitude'].iloc[0]], \n",
    "               zoom_start=15)\n",
    "\n",
    "# Adiciona os pontos e linhas ao mapa\n",
    "for i in range(len(df_geracao) - 1):\n",
    "    # Coordenadas do ponto atual e do próximo ponto\n",
    "    lat1, lon1 = df_geracao.iloc[i][['new_latitude', 'new_longitude']]\n",
    "    lat2, lon2 = df_geracao.iloc[i + 1][['new_latitude', 'new_longitude']]\n",
    "    \n",
    "    # Classificação do ponto atual\n",
    "    classificacao = df_geracao.iloc[i]['classificacao']\n",
    "    \n",
    "    # Define a cor da linha com base na classificação\n",
    "    line_color = 'red' if classificacao == 0 else 'green'\n",
    "    \n",
    "    # Adiciona uma linha entre o ponto atual e o próximo\n",
    "    folium.PolyLine([(lat1, lon1), (lat2, lon2)], color=line_color, weight=5).add_to(m)\n",
    "    \n",
    "    # Adiciona um marcador no ponto atual\n",
    "    #folium.Marker(\n",
    "    #    location=[lat1, lon1],\n",
    "    #    popup=f\"Coordenadas: ({lat1}, {lon1})<br>Classificação: {classificacao}\",\n",
    "    #   icon=folium.Icon(color='blue', icon='info-sign')\n",
    "    #).add_to(m)\n",
    "\n",
    "# Salva o mapa em um arquivo HTML e exibe o mapa\n",
    "output_file = r\"C:\\Users\\vidal\\Downloads\\TCC\\ClassificacaoCondutores\\percurso_final.html\"\n",
    "m.save(output_file)\n",
    "output_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
